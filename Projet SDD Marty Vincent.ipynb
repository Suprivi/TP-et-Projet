{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Science des données Marty Vincent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N° étudiant : 21704218"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonjour, dans ce notebook, nous allons examiner deux ensembles de données. Le premier concerne les risques d'AVC, où nous effectuerons une analyse exploratoire et utiliserons une régression logistique pour créer un modèle prédictif. Le deuxième concerne les clients d'un supermarché, où nous appliquerons la méthode Kmean et la compléterons avec une analyse en composantes principales (ACP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set()\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('expand_frame_repr', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'healthcare-dataset-stroke-data.csv'\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Exploratoire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par regarder ce avec quoi nous travaillons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {df.shape[0]} observations and {df.shape[1]} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns.tolist():\n",
    "    print('le type de la colonne',i ,' est:',df[i].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(df['stroke'].value_counts(), labels = ['no stroke','stroke'], autopct='%1.1f%%');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Remarque}$\n",
    "\n",
    "Il semble y avoir un déséquilibre dans nos données, ce qui peut avoir un impact sur la performance de notre modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse des données manquantes dans chaque colonne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_missing(x):\n",
    "    return sum(x.isnull())\n",
    "\n",
    "print(\"Valeurs manquantes par colonne:\")\n",
    "\n",
    "print(df.apply(num_missing, axis=0).where(lambda x : x != 0).dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il manque 201 valeurs dans la colonne BMI, pour savoir ce que nous pouvons faire de ces données manquantes, regardons si il y a un biais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.pie(df['stroke'].value_counts(), labels = ['no stroke','stroke'], autopct='%1.1f%%')\n",
    "plt.title('BMI connu');\n",
    "plt.subplot(1,2,2)\n",
    "plt.pie(df['stroke'].where(df['bmi'].isnull()).value_counts(), labels = ['no stroke','stroke'], autopct='%1.1f%%');\n",
    "plt.title('BMI inconnu');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a un clair déséquilibre entre les deux graphiques, on peut donc estimer que les données manquantes sont biaisés. On va remplacer les données manquantes par la moyenne de la colonne 'BMI'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bmi'] = df['bmi'].fillna(df['bmi'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.apply(num_missing, axis=0).where(lambda x : x != 0).dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les valeurs manquantes ont bien été remplacés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On enlève les colonnes où chacune des valeurs sont uniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if df[col].unique().size==df.shape[0]:\n",
    "        print(\"Dropping column: {0}\".format(col))\n",
    "        df = df.drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etudes des données numériques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons ici regarder la distributions des différentes variables numériques, c'est à dire : 'age', 'bmi' et 'avg_glucose_level'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(df['age'],shade = True,edgecolor='black').text(0,0.0175,\"Répartitions de l'age dans le jeu de donnée\",size = 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On note qu'une grande partie des personnes dans ce jeu de donnée on entre 40 et 60 ans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(df['age'].where(df['stroke'] == 0),label = 'no stroke',shade = True,edgecolor='black')\n",
    "sns.kdeplot(df['age'].where(df['stroke'] == 1),label = 'stroke',shade = True,edgecolor='black').text(-10,0.043,\"Répartitions de l'age dans le jeu de donnée en fonction de l'AVC\",size = 20);\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'age semble être une variable importante pour expliquer le risque d'AVC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(df['bmi'].where(df['stroke'] == 0),label = 'no stroke',shade = True,edgecolor='black');\n",
    "sns.kdeplot(df['bmi'].where(df['stroke'] == 1),label = 'stroke',shade = True,edgecolor='black');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il semble que la colonne 'bmi' explique peu le risque d'AVC, cela pourrait être dû à des données manquantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(df['avg_glucose_level'].where(df['stroke'] == 0),label = 'no stroke',shade = True,edgecolor='black');\n",
    "sns.kdeplot(df['avg_glucose_level'].where(df['stroke'] == 1),label = 'stroke',shade = True,edgecolor='black');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il semble que les personnes ayant un niveau de glucose plus élevé ont un plus grand risque d'AVC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des variables catégorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p = sns.barplot(x='gender',y = 'stroke',data = df, estimator = np.mean);\n",
    "p.set(title=\"Proportion d'AVC par rapport au genre\")\n",
    "p.set_xticklabels(p.get_xticklabels(), rotation=-45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable 'gender' semble avoir peu d'effet sur le risque d'AVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.barplot(x='ever_married',y = 'stroke',data = df, estimator = np.mean);\n",
    "p.set(title=\"Proportion d'AVC par rapport au mariage\")\n",
    "p.set_xticklabels(p.get_xticklabels(), rotation=-45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable 'ever_married' semble avoir beaucoup d'effet sur le risque d'AVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.barplot(x='heart_disease',y = 'stroke',data = df, estimator = np.mean);\n",
    "p.set(title=\"Proportion d'AVC par rapport à la présence de maladie au coeur\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable 'heart_disease' semble avoir beaucoup d'effet sur le risque d'AVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.barplot(x='hypertension',y = 'stroke',data = df, estimator = np.mean);\n",
    "p.set(title=\"Proportion d'AVC par rapport à l'hypertension\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable 'hypertension' semble avoir beaucoup d'effet sur le risque d'AVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.barplot(x='smoking_status',y = 'stroke',data = df, estimator = np.mean);\n",
    "p.set(title=\"Proportion d'AVC par rapport au tabagisme\");\n",
    "p.set_xticklabels(p.get_xticklabels(), rotation=-45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les personnes qui fume ou qui ont fumé semble être les plus à risque d'AVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.barplot(x='Residence_type',y = 'stroke',data = df, estimator = np.mean);\n",
    "p.set(title=\"Proportion d'AVC par rapport au type de résidence\");\n",
    "p.set_xticklabels(p.get_xticklabels(), rotation=-45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable 'Residence_type' semble avoir peu d'effet sur le risque d'AVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.barplot(x='work_type',y = 'stroke',data = df, estimator = np.mean);\n",
    "p.set(title=\"Proportion d'AVC par rapport au type de travail\");\n",
    "p.set_xticklabels(p.get_xticklabels(), rotation=-45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les indépendants semble les plus touché par les risques d'AVC, il faut faire attention toutefois on peut remarquer la présence de la variable 'children' ce qui peut indiquer une certaines corrélation avec la variable 'age'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage supervisé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On souhaite prédire la possibilité qu'une personne est un AVC. La colonne AVC est binaire donc pour créer un modèle d'apprentissage, on va utiliser un modèle de régression logistique. (Une explication plus détaillé [sur cette vidéo](https://www.youtube.com/watch?v=yIYKR4sgzI8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour utiliser un modèle multivarié il faut d'abord vérifier la corrélation entre chaque variable pour éviter tout problème de multicolinéarité. On va utiliser la fonction heatmap de seaborn pour le visualiser, mais avant tout il faut encoder les variables catégorielles et normaliser les variables numériques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats as stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, recall_score, precision_score, roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "for col in df_copy.columns.tolist():\n",
    "    if df_copy[col].dtype=='object':\n",
    "        #On encode les variables catégorielles à l'aide de LabelEncoder\n",
    "        df_copy[col] = preprocessing.LabelEncoder().fit_transform(df_copy[col])\n",
    "\n",
    "\n",
    "Z = ['age','bmi','avg_glucose_level']\n",
    "#On normalise les variables numériques 'age', 'bmi' et 'avg_glucose_level'\n",
    "df_copy[Z] = preprocessing.MinMaxScaler().fit_transform(df_copy[Z])\n",
    "df_copy.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les variables catégorielles ont bien été encodées et les variables numériques ont bien été normalisées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_copy.corr(),annot = True,fmt=\".2f\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut faire plusieurs observation:\n",
    "- 'age' est bien la variable la plus importante pour prédire le risque d'AVC.\n",
    "- 'ever_married' qui semblait être une variable importante pour prédire le risque d'AVC est aussi corrélée avec la variable 'age', donc nous ne l'utiliserons pas.\n",
    "- 'heart_disease', 'hypertension' et 'avg_glucose_level' semble avoir une importance significative pour prédire le risque d'AVC.\n",
    "- Le reste des variables ne semble pas avoir une importance suffisantes pour prédire le risque d'AVC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Conclusion :}$\n",
    "\n",
    "On utilisera pour notre modèle de régression logistique la colonne 'age', 'heart_disease', 'hypertension' et 'avg_glucose_level'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# Define our dependent variable\n",
    "y_column = 'stroke'\n",
    "y = df_copy[y_column]\n",
    "\n",
    "# Define our independent variables\n",
    "# After testing, it appears that heart_disease lower the performance of our model, so we will not use it.\n",
    "x_columns = ['age','hypertension','avg_glucose_level']\n",
    "X = df_copy[x_columns]\n",
    "\n",
    "# Add an intercept term to the independent variables. This is needed in order to include the constant term from\n",
    "# logistic regression equation.\n",
    "X['intercept'] = 1\n",
    "\n",
    "# Split our data into training and test data sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "y_pred = logistic_regression.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre modèle c'est bien entrainé sur les valeurs d'entrainement, nous allons regardé à présent son efficacité sur les valeur de test. Pour cela on va utilisé de façon de comparer les résultat:\n",
    "- Précision: La précision mesure la capacité du modèle à prédire correctement les cas positifs.\n",
    "- TPR: Le TPR (Taux de vrais positifs) mesure la proportion de cas positifs réels qui sont correctement prédits par le modèle.\n",
    "- F1 Score: Il s'agit d'un indicateur de performance qui combine la précision et le TPR en utilisant une moyenne pondérée. Il peut être utile lorsque la distribution des classes est déséquilibrée.\n",
    "- Exactitude: L'exactitude mesure la proportion de prédictions correctes par rapport au nombre total de prédictions.\n",
    "- L'AUC-ROC: Aire sous la courbe ROC mesure la performance du modèle en comparant les scores prédits aux valeurs réelles. Si le score est de 0.5, alors notre modèle est aussi efficace qu'un modèle aléatoire. On souhaite que notre score soit le plus proche de 1. On peut trouver une explication plus détaillé [ici](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc?hl=fr).\n",
    "- La matrice de confusion, Il s'agit d'un tableau qui montre le nombre de vrais positifs, de faux positifs, de vrais négatifs et de faux négatifs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "tpr = recall_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 score: \", f1)\n",
    "print(\"Précision: {:.3f}\".format(precision))\n",
    "print(\"TPR: {:.3f}\".format(tpr))\n",
    "print(\"Exactitude: {:.3f}\".format(accuracy))\n",
    "print(\"AUC-ROC: {:.3f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons un déséquilibre dans nos données entre 'stroke' et 'no stroke', ce qui empêche notre modèle de s'entraîner correctement. Pour corriger ce problème, nous utiliserons la technique d'oversampling SMOTE de la librairie imblearn (un article plus détaillé [ici](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)). Notez que cette librairie doit être installée manuellement via le terminal.\n",
    "- Pour anaconda : conda install -c conda-forge imbalanced-learn\n",
    "- Pour PyPi : pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "X_train_smo, y_train_smo = SMOTE().fit_resample(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(X_train_smo, y_train_smo)\n",
    "y_pred = logistic_regression.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "tpr = recall_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 score: {:.3f}\".format(f1))\n",
    "print(\"Précision: {:.3f}\".format(precision))\n",
    "print(\"TPR: {:.3f}\".format(tpr))\n",
    "print(\"Exactitude: {:.3f}\".format(accuracy))\n",
    "print(\"AUC-ROC: {:.3f}\".format(roc_auc))\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues');\n",
    "plt.xlabel('Valeurs Prédites')\n",
    "plt.ylabel('Valeurs Réelle')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe que notre modèle s'est considérablement amélioré, mais n'est pas encore parfait. Le taux de faux négatifs a significativement diminué, ce qui est crucial dans le domaine médical. Cependant, avec un F1 score d'environ 0,25, une exactitude d'environ 0.75 et un AUC-ROC d'environ 0.8 , notre modèle peut être utile mais ne peut pas remplacer un expert. Nous devons effectuer une dernière vérification des données de test pour éviter tout overfitting. Pour cela, nous utiliserons le score de validation croisée et la courbe d'apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score de validation croisée et courbe d'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le score de validation croisée (cross-validation score) est une méthode utilisée pour évaluer la performance d'un modèle de machine learning sur des données non vues. Il fonctionne en divisant les données en plusieurs parties, en entraînant le modèle sur une partie et en l'évaluant sur une autre partie. Ce processus est répété plusieurs fois avec des parties différentes utilisées à chaque fois pour l'entraînement et l'évaluation. La performance finale du modèle est déterminée par la moyenne des scores obtenus à chaque itération. Cela permet d'obtenir une estimation plus fiable de la performance du modèle sur des données réelles. Cette méthode peut être utile pour détecter les possibilité d'overfitting ou d'underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, learning_curve\n",
    "scores = cross_val_score(logistic_regression, X_train_smo, y_train_smo, cv=5)\n",
    "print(\"Scores de validation croisée:\", scores)\n",
    "print(\"Moyenne du score de validation croisée:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec des scores de validation croisée proches les uns des autres et proche de la moyenne, il semblerait à première vue que notre modèle donne des résultats satisfaisants sans présenter d'overfitting. Nous allons tout de même faire une deuxième vérification avec la courbe d'apprentissage pour être sûre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La courbe d'apprentissage est un graphique qui montre la relation entre la performance d'un modèle de machine learning et la quantité de données utilisées pour l'entraîner. Plus de détail sur ce graphique sur [ce lien](https://vitalflux.com/learning-curves-explained-python-sklearn-example/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(estimator=logistic_regression,\n",
    "                                                        X=X_train_smo,\n",
    "                                                        y=y_train_smo,\n",
    "                                                        cv=5,\n",
    "                                                        n_jobs=-1,\n",
    "                                                        #nombre de point dans la courbe, il vaut mieux mettre un nombre paire\n",
    "                                                        #puisque ces poitns sont partagés entre les deux courbes.\n",
    "                                                        train_sizes=np.linspace(0.1, 1.0, 20))\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label=\"Précision d'apprentissage\")\n",
    "plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, test_mean, color='green', linestyle='--', marker='s', markersize=5, label='Précision de validation')\n",
    "plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel(\"Nombre d'échantillons d'apprentissage\")\n",
    "plt.ylabel('Précision')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.title(\"Courbe d'apprentissage\")\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec cette courbe on peut en conclure que notre modèle possède un bon équilibre entre biais et variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce modèle basé sur une régression logistique présente une précision acceptable sans risque d'overfitting, cependant, il peut encore être amélioré pour une utilisation réelle. Pour obtenir un modèle plus performant, il y a plusieurs options à explorer, telles que l'utilisation d'une autre méthode d'apprentissage, comme les forêts aléatoires, ou une meilleure base de données avec un nombre équilibré de cas d'AVC dès le début."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage non supervisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Mall_Customers.csv'\n",
    "df2 = pd.read_csv(filename)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {df2.shape[0]} observations and {df2.shape[1]} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valeurs manquantes par colonne:\")\n",
    "print(df2.apply(num_missing, axis=0).where(lambda x : x != 0).dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aucune donnée manquante, on peut continuer sans problème."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df2.columns.tolist():\n",
    "    print('le type de la colonne',i ,' est:',df2[i].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va enlever la colonne 'CustomerID' qui n'a aucune utilité pour la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df2.drop('CustomerID', axis=1), hue = 'Gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il semble il y a avoir peut de différence en fonction du genre, on va donc supprimé la colonne gender et customerID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_data = df2.drop(['CustomerID','Gender'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage\n",
    "from pylab import rcParams\n",
    "\n",
    "#generate the linkage matrix – Ward method\n",
    "Z = linkage(df2_data,method='ward',metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va créer un Elbow plot pour déterminer le nombre de cluster K que l'on va utiliser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to store WCSS\n",
    "wcss = []\n",
    "\n",
    "# for loop\n",
    "for i in range(1, 11):\n",
    "    # k-mean cluster model for different k values\n",
    "    kmeans=KMeans(n_clusters=i,random_state = 0)\n",
    "    kmeans.fit(df2_data)\n",
    "    # inertia method returns wcss for that model\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "# figure size\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.lineplot(range(1, 11), wcss,marker='o',color='green')\n",
    "\n",
    "# labeling\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut prendre K = 5 pour notre Kmean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster K-means\n",
    "model=KMeans(n_clusters=5,random_state = 42)\n",
    "#adapter le modèle de données\n",
    "model.fit(df2_data)\n",
    "res_label = model.labels_\n",
    "print(res_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On ajoute une colonne qui contiendra le cluster choisi par kmeans\n",
    "df2.insert(0, \"kmeansClass\", model.labels_, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = df2[\"Annual Income (k$)\"]\n",
    "x2 = df2[\"Age\"]\n",
    "y = df2[\"Spending Score (1-100)\"]\n",
    "\n",
    "colors = {0:'red', 1:'blue', 2:'green', 3:'yellow', 4:'black'}\n",
    "colors_c = [colors[cl] for cl in df2[\"kmeansClass\"]]\n",
    "\n",
    "plt.scatter(x1, y, c=colors_c)\n",
    "plt.title(\"KMeans Clusters\")\n",
    "plt.xlabel(\"Annual Income (k$)\")\n",
    "plt.ylabel(\"Spending Score (1-100)\")\n",
    "plt.show();\n",
    "\n",
    "plt.scatter(x2, y, c=colors_c)\n",
    "plt.title(\"KMeans Clusters\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Spending Score (1-100)\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Commentaire :}$\n",
    "\n",
    "On peut classer les clusters en utilisant les 2 graphiques:\n",
    "\n",
    "- Cluster vert: jeunes clients avec bas salaire mais forte dépense\n",
    "- Cluster jaune: jeunes clients avec bon salaire et forte dépense\n",
    "- Cluster rouge: clients de tout âge avec bon salaire mais faible dépense\n",
    "- Cluster noir: clients de tout âge avec bas salaire et faible dépense\n",
    "- Cluster bleu: clients de tout âge avec salaire moyen et dépense moyenne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "pca = PCA()\n",
    "pca.fit(df2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig = pd.DataFrame(\n",
    "{\n",
    "\"Dimension\" : [\"Dim\" + str(x + 1) for x in range(3)],\n",
    "\"Variance expliquée\" : pca.explained_variance_,\n",
    "\"% variance expliquée\" : np.round(pca.explained_variance_ratio_ * 100),\n",
    "\"% cum. var. expliquée\" : np.round(np.cumsum(pca.explained_variance_ratio_) * 100)\n",
    "}\n",
    ")\n",
    "eig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] =10, 10\n",
    "eig.plot.bar(x = \"Dimension\", y = \"% variance expliquée\").text(2.2, 34, \"33%\") # ajout de texte\n",
    "# permet un diagramme en barres\n",
    "plt.axhline(y = 33, linewidth = .5, color = \"dimgray\", linestyle = \"--\") \n",
    "# ligne 33 = 100 / 3 (nb dimensions)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque qu'avec 2 dimensions on arrive à expliquer 89 % de la variance des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_pca = pca.transform(df2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.insert(0, \"pca_dim1\", df2_pca[:,0], True)\n",
    "df2.insert(0, \"pca_dim2\", df2_pca[:,1], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_pca_df = pd.DataFrame({\n",
    "\"Dim1\" : df2[\"pca_dim1\"],\n",
    "\"Dim2\" : df2[\"pca_dim2\"],\n",
    "\"Spending Score (1-100)\" : df2[\"Spending Score (1-100)\"],\n",
    "})\n",
    "df2_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_pca_df.plot.scatter(\"Dim1\", \"Dim2\") # nuage de points\n",
    "plt.xlabel(\"Dimension 1 (45%)\") # modification du nom de l'axe X\n",
    "plt.ylabel(\"Dimension 2 (44%)\") # idem pour axe Y\n",
    "plt.suptitle(\"Premier plan factoriel (89%)\") # titre général\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour classer nos points on va rajouter une nouvelle colonne à notre dataset qui représentera les différents niveaux de dépenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depense(spending_score):\n",
    "    if spending_score < 33:\n",
    "        return \"Dépense Faible\"\n",
    "    elif spending_score >= 33 and spending_score <= 66:\n",
    "        return \"Dépense Moyenne\"\n",
    "    else:\n",
    "        return \"Dépense Elevé\"\n",
    "\n",
    "df2_pca_df['Dépense'] = df2_pca_df['Spending Score (1-100)'].apply(depense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permet de créer une palette de couleurs, basée sur Color Brewer\n",
    "palette = plt.get_cmap(\"Dark2\")\n",
    "couleurs = dict(zip(df2_pca_df[\"Dépense\"].drop_duplicates(),palette(range(3))))\n",
    "couleurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = dict(zip(couleurs.keys(), range(3)))\n",
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des points avec une liste de couleurs\n",
    "df2_pca_df.plot.scatter(x = \"Dim1\", y = \"Dim2\", c = [couleurs[p] for p in df2_pca_df[\"Dépense\"]])\n",
    "# boucle pour afficher la légende\n",
    "for cont, coul in couleurs.items():\n",
    "    plt.scatter(55, 7*position[cont]+60, c = [coul], s = 20)\n",
    "    plt.text(57, 7*position[cont]-2 + 60, cont)\n",
    "plt.xlabel(\"Dimension 1 (45%)\")\n",
    "plt.ylabel(\"Dimension 2 (44 %)\")\n",
    "plt.suptitle(\"Premier plan factoriel (89%)\")\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = df2_data.shape[0] # nb individus\n",
    "p = df2_data.shape[1] # nb variables\n",
    "eigval = (n-1) / n * pca.explained_variance_ # valeurs propres\n",
    "sqrt_eigval = np.sqrt(eigval) # racine carrée des valeurs propres\n",
    "corvar = np.zeros((p,p)) # matrice vide pour avoir les coordonnées\n",
    "for k in range(p):\n",
    "    corvar[:,k] = pca.components_[k,:] * sqrt_eigval[k]\n",
    "# on modifie pour avoir un dataframe\n",
    "coordvar = pd.DataFrame({'id': df2_data.columns , 'COR_1': corvar[:,0], 'COR_2': corvar[:,1]})\n",
    "coordvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'axe 1 semble opposé les personnes en fonction de l'âge.\n",
    "\n",
    "L'axe 2 semble opposé les personnes avec un faible revenu au personnes avec un fort revenu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
